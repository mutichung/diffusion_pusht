{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>This is the documentation of my 7-day challenge on training a diffusion policy on the PushT task.</p>"},{"location":"#overview","title":"Overview","text":"<ul> <li>Home contains basic info like environment setup, etc.</li> <li>Experiments holds the experiment details, results, and analyses.</li> <li>Logs are where I record my daily progress and random notes.</li> </ul>"},{"location":"setup/","title":"Setup","text":""},{"location":"setup/#dependency-installation-and-repository-setup","title":"Dependency Installation and Repository Setup","text":"<p>First, clone the repository.</p> <pre><code>git clone --recursive https://github.com/mutichung/diffusion_pusht.git\ncd diffusion_pusht\n</code></pre> <p>The original author suggests using <code>mamba</code> or <code>conda</code> for environment creation.</p> <pre><code>mamba create -p ./env -f diffusion_policy/conda_environment.yaml\nmamba activate ./env\n\n# Pin version to avoid ImportError(\"cannot import name 'cached_download' from 'huggingface_hub'\").\npip install huggingface_hub==0.11.1\n\n# Uncomment this if you want to build the documentation.\n# pip install mkdocs-material\n</code></pre>"},{"location":"setup/#patching-and-installing-real-stanforddiffusion_policy","title":"Patching and Installing <code>real-stanford/diffusion_policy</code>","text":"<p>[!NOTE] The original <code>real-stanford/diffusion_policy</code> repository uses a flat layout instead of a more standard, <code>src</code>-based one. In addition, the <code>__init__.py</code> files are not presented in any subdirectories that contain python files <sup>1</sup>. This makes it difficult to install the repository as a package using <code>pip install</code>. Therefore, I created the missing <code>__init__.py</code> files (plus some other changes) and save them as patches for others to use.</p> <p>Apply the patches and install the repository as a package using the following commands.</p> <pre><code>git am patches/*\npip install -e diffusion_policy\n</code></pre>"},{"location":"setup/#documentation","title":"Documentation","text":"<p>All documentation is written in plain markdown. MkDocs is used as the static site generator (SSG) to build the documentation. The theme used is Material for MkDocs.</p> <p>To build it for deployment, run the following command.</p> <pre><code>pip install mkdocs-material\nmkdocs build\n</code></pre> <p>The documentation is then available at <code>site/index.html</code>.</p> <ol> <li> <p>I suspect this is because the original author is using <code>hydra</code> for configuration management.\u00a0\u21a9</p> </li> </ol>"},{"location":"experiments/data_analysis/","title":"Dataset Analysis","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport base64\nfrom typing import Optional, Sequence\n\nimport zarr\nimport imageio\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tabulate\nfrom IPython.display import HTML\nfrom shapely import geometry\nfrom shapely import affinity\nfrom tqdm import tqdm\n</pre> import os import base64 from typing import Optional, Sequence  import zarr import imageio import matplotlib.pyplot as plt import numpy as np import tabulate from IPython.display import HTML from shapely import geometry from shapely import affinity from tqdm import tqdm   <p>Be sure to download and extract the dataset under <code>diffusion_pusht/data/pusht</code>.</p> In\u00a0[2]: Copied! <pre>data_v1 = zarr.open_group(\"../../data/pusht/pusht_cchi_v1.zarr\")\ndata_v2 = zarr.open_group(\"../../data/pusht/pusht_cchi_v2.zarr/\")\n</pre> data_v1 = zarr.open_group(\"../../data/pusht/pusht_cchi_v1.zarr\") data_v2 = zarr.open_group(\"../../data/pusht/pusht_cchi_v2.zarr/\") In\u00a0[3]: Copied! <pre>def get_length(data) -&gt; int:\n    return len(data[\"meta\"][\"episode_ends\"][:].tolist())\n\n\ndef get_element_by_idx(data, key: str, idx: int):\n    if idx &gt;= get_length(data):\n        raise IndexError\n    episode_ends = data[\"meta\"][\"episode_ends\"][:].tolist()\n    start = 0 if idx == 0 else episode_ends[idx - 1]\n    end = episode_ends[idx]\n    return data[\"data\"][key][start:end]\n\n\ndef get_video_by_idx(data, idx: int):\n    return np.array(get_element_by_idx(data, \"img\", idx), dtype=np.uint8)\n\n\ndef get_identical_indices(data_v1, data_v2):\n    \"\"\"Find out which episode in v2 corresponds to v1.\"\"\"\n    v2_idx_set = set(range(get_length(data_v2)))\n    v1_to_v2 = {}\n\n    for i in range(get_length(data_v1)):\n        imgs1 = get_video_by_idx(data_v1, i)\n        for j in v2_idx_set:\n            imgs2 = get_video_by_idx(data_v2, j)\n            if imgs1.shape[0] != imgs2.shape[0]:\n                continue\n            diff = np.abs(imgs1 - imgs2).sum()\n            if diff == 0:\n                v1_to_v2.update({i: j})\n                v2_idx_set.remove(j)\n                break\n    return v1_to_v2\n\n\nv1_to_v2 = get_identical_indices(data_v1, data_v2)\nv2_exclusives = set(range(get_length(data_v2))) - set(v1_to_v2.values())\n\nprint(f\"Number of identical episodes: {len(v1_to_v2)}\")\nprint(f\"Ratio of identical episodes: {len(v1_to_v2) / get_length(data_v1) * 100:.0f}%\")\n</pre> def get_length(data) -&gt; int:     return len(data[\"meta\"][\"episode_ends\"][:].tolist())   def get_element_by_idx(data, key: str, idx: int):     if idx &gt;= get_length(data):         raise IndexError     episode_ends = data[\"meta\"][\"episode_ends\"][:].tolist()     start = 0 if idx == 0 else episode_ends[idx - 1]     end = episode_ends[idx]     return data[\"data\"][key][start:end]   def get_video_by_idx(data, idx: int):     return np.array(get_element_by_idx(data, \"img\", idx), dtype=np.uint8)   def get_identical_indices(data_v1, data_v2):     \"\"\"Find out which episode in v2 corresponds to v1.\"\"\"     v2_idx_set = set(range(get_length(data_v2)))     v1_to_v2 = {}      for i in range(get_length(data_v1)):         imgs1 = get_video_by_idx(data_v1, i)         for j in v2_idx_set:             imgs2 = get_video_by_idx(data_v2, j)             if imgs1.shape[0] != imgs2.shape[0]:                 continue             diff = np.abs(imgs1 - imgs2).sum()             if diff == 0:                 v1_to_v2.update({i: j})                 v2_idx_set.remove(j)                 break     return v1_to_v2   v1_to_v2 = get_identical_indices(data_v1, data_v2) v2_exclusives = set(range(get_length(data_v2))) - set(v1_to_v2.values())  print(f\"Number of identical episodes: {len(v1_to_v2)}\") print(f\"Ratio of identical episodes: {len(v1_to_v2) / get_length(data_v1) * 100:.0f}%\")  <pre>Number of identical episodes: 103\nRatio of identical episodes: 100%\n</pre> <p>Seems like v1 is a subset of v2!</p> <p>Now that v2 is an extension of v1, I wanted to see if the additional episodes in v2 share the same initial states as v1. If the initial states are identical, it might be easier to assess the quality of the two datasets.</p> In\u00a0[4]: Copied! <pre>def match_identical_initial_state(data_v1, data_v2):\n    \"\"\"Find out whether there are identical initial states.\"\"\"\n    matches = []\n    for i in range(get_length(data_v1)):\n        curr = set()\n        s1 = get_element_by_idx(data_v1, \"state\", i)[0, :]\n        for j in range(get_length(data_v2)):\n            s2 = get_element_by_idx(data_v2, \"state\", j)[0, :]\n            if (s1 == s2).all():\n                curr.add(j)\n        matches.append(curr)\n    return matches\n\ninit_cond_matches = match_identical_initial_state(data_v1, data_v2)\n\nif any(len(matches) &gt; 1 for matches in init_cond_matches):\n    print(\"There are identical initial states in v2.\")\nelse:\n    print(\"No identical initial states in v2 are found.\")\n</pre> def match_identical_initial_state(data_v1, data_v2):     \"\"\"Find out whether there are identical initial states.\"\"\"     matches = []     for i in range(get_length(data_v1)):         curr = set()         s1 = get_element_by_idx(data_v1, \"state\", i)[0, :]         for j in range(get_length(data_v2)):             s2 = get_element_by_idx(data_v2, \"state\", j)[0, :]             if (s1 == s2).all():                 curr.add(j)         matches.append(curr)     return matches  init_cond_matches = match_identical_initial_state(data_v1, data_v2)  if any(len(matches) &gt; 1 for matches in init_cond_matches):     print(\"There are identical initial states in v2.\") else:     print(\"No identical initial states in v2 are found.\")  <pre>No identical initial states in v2 are found.\n</pre> In\u00a0[5]: Copied! <pre>GOAL_POSE = np.array([256, 256, np.pi / 4])\nLENGTH = 4\nSCALE = 30\nSUCCESS_THRESHOLD = 0.95\n\n\ndef get_tee(position: tuple[float, float], angle: float):\n    \"\"\"Adopted from diffusion_policy source code.\"\"\"\n    tee = geometry.Polygon([\n        (-LENGTH * SCALE / 2, 0),\n        (-LENGTH * SCALE / 2, SCALE),\n        (-SCALE / 2, SCALE),\n        (-SCALE / 2, SCALE * LENGTH),\n        (SCALE / 2, SCALE * LENGTH),\n        (SCALE / 2, SCALE),\n        (LENGTH * SCALE / 2, SCALE),\n        (LENGTH * SCALE / 2, 0),\n        (0, 0),\n    ])\n\n    return affinity.translate(\n        affinity.rotate(tee, angle, origin=(0,0), use_radians=True),\n        position[0],\n        position[1]\n    )\n\n\ndef get_goal():\n    return get_tee((256, 256), np.pi / 4)\n\n\ndef get_metric(data, idx: int):\n    goal = get_goal()\n    goal_area = goal.area\n    states = get_element_by_idx(data, \"state\", idx)\n    max_reward = 0\n    max_converage = 0\n    for state in states:\n        curr = get_tee((state[2], state[3]), state[4])\n        intersect = goal.intersection(curr).area\n        coverage = intersect / goal_area\n        reward = np.clip(coverage / SUCCESS_THRESHOLD, 0, 1)\n        max_reward = max(max_reward, reward)\n        max_converage = max(max_converage, coverage)\n\n    is_success = (max_converage &gt;= SUCCESS_THRESHOLD)\n\n    return max_reward, is_success\n\n\ndef get_mean_metrics(data, indices: Optional[Sequence[int]] = None):\n    total_reward = 0\n    total_success = 0\n    if indices is None:\n        indices = list(range(get_length(data)))\n\n    for i in indices:\n        reward, is_success = get_metric(data, i)\n        total_reward += reward\n        if is_success:\n            total_success += 1\n\n    return total_reward / len(indices), total_success / len(indices)\n\nreward_v1, success_rate_v1 = get_mean_metrics(data_v1)\nreward_v2, success_rate_v2 = get_mean_metrics(data_v2)\nreward_v2_exclusive, success_rate_v2_exclusive = get_mean_metrics(data_v2, v2_exclusives)\n\nprint(\n    tabulate.tabulate(\n        [\n            [\"v1\", reward_v1, success_rate_v1],\n            [\"v2\", reward_v2, success_rate_v2],\n            [\"v2_exclusive\", reward_v2_exclusive, success_rate_v2_exclusive],\n        ],\n        headers=[\"Dataset\", \"Mean Reward\", \"Success Rate\"],\n        tablefmt=\"github\",\n    )\n)\n</pre> GOAL_POSE = np.array([256, 256, np.pi / 4]) LENGTH = 4 SCALE = 30 SUCCESS_THRESHOLD = 0.95   def get_tee(position: tuple[float, float], angle: float):     \"\"\"Adopted from diffusion_policy source code.\"\"\"     tee = geometry.Polygon([         (-LENGTH * SCALE / 2, 0),         (-LENGTH * SCALE / 2, SCALE),         (-SCALE / 2, SCALE),         (-SCALE / 2, SCALE * LENGTH),         (SCALE / 2, SCALE * LENGTH),         (SCALE / 2, SCALE),         (LENGTH * SCALE / 2, SCALE),         (LENGTH * SCALE / 2, 0),         (0, 0),     ])      return affinity.translate(         affinity.rotate(tee, angle, origin=(0,0), use_radians=True),         position[0],         position[1]     )   def get_goal():     return get_tee((256, 256), np.pi / 4)   def get_metric(data, idx: int):     goal = get_goal()     goal_area = goal.area     states = get_element_by_idx(data, \"state\", idx)     max_reward = 0     max_converage = 0     for state in states:         curr = get_tee((state[2], state[3]), state[4])         intersect = goal.intersection(curr).area         coverage = intersect / goal_area         reward = np.clip(coverage / SUCCESS_THRESHOLD, 0, 1)         max_reward = max(max_reward, reward)         max_converage = max(max_converage, coverage)      is_success = (max_converage &gt;= SUCCESS_THRESHOLD)      return max_reward, is_success   def get_mean_metrics(data, indices: Optional[Sequence[int]] = None):     total_reward = 0     total_success = 0     if indices is None:         indices = list(range(get_length(data)))      for i in indices:         reward, is_success = get_metric(data, i)         total_reward += reward         if is_success:             total_success += 1      return total_reward / len(indices), total_success / len(indices)  reward_v1, success_rate_v1 = get_mean_metrics(data_v1) reward_v2, success_rate_v2 = get_mean_metrics(data_v2) reward_v2_exclusive, success_rate_v2_exclusive = get_mean_metrics(data_v2, v2_exclusives)  print(     tabulate.tabulate(         [             [\"v1\", reward_v1, success_rate_v1],             [\"v2\", reward_v2, success_rate_v2],             [\"v2_exclusive\", reward_v2_exclusive, success_rate_v2_exclusive],         ],         headers=[\"Dataset\", \"Mean Reward\", \"Success Rate\"],         tablefmt=\"github\",     ) )  <pre>| Dataset      |   Mean Reward |   Success Rate |\n|--------------|---------------|----------------|\n| v1           |      0.893062 |              0 |\n| v2           |      0.8923   |              0 |\n| v2_exclusive |      0.891538 |              0 |\n</pre> <p>From the table above, it seems that dataset v1 and v2 have similar quality, both in terms of reward and success rate. There isn't a significant difference in the metrics between v1 and v2. As a result, this might be an implication that the result of my initial test runs were not because of data quality differences but the lack of sufficient training epochs.</p> <p>Below is a visualization of the current &amp; goal poses to ensure that the calculations are valid.</p> In\u00a0[6]: Copied! <pre>def plot_kp(points, goal, kp):\n    # Plot the points\n    plt.figure(figsize=(6, 6))\n    plt.plot(points[:, 0], -points[:, 1], color='blue')\n    plt.plot(goal[:, 0], -goal[:, 1], color=\"green\")\n    plt.scatter(kp[:, 0], -kp[:, 1], color=\"cyan\")\n\n    plt.title(\"Labeled Points from (9, 2) Array\")\n    plt.xlabel(\"X\")\n    plt.ylabel(\"Y\")\n    plt.grid(True)\n    plt.axis('equal')  # Keep aspect ratio equal for better visualization\n    plt.show()\n\n\nstates = get_element_by_idx(data_v1, \"state\", 0)\nstate = states[-1]\ncurr = get_tee((state[2], state[3]), state[4])\nkp = get_element_by_idx(data_v1, \"keypoint\", 0)[-1]\nplot_kp(np.array(curr.exterior.coords), np.array(get_goal().exterior.coords), kp)\n</pre> def plot_kp(points, goal, kp):     # Plot the points     plt.figure(figsize=(6, 6))     plt.plot(points[:, 0], -points[:, 1], color='blue')     plt.plot(goal[:, 0], -goal[:, 1], color=\"green\")     plt.scatter(kp[:, 0], -kp[:, 1], color=\"cyan\")      plt.title(\"Labeled Points from (9, 2) Array\")     plt.xlabel(\"X\")     plt.ylabel(\"Y\")     plt.grid(True)     plt.axis('equal')  # Keep aspect ratio equal for better visualization     plt.show()   states = get_element_by_idx(data_v1, \"state\", 0) state = states[-1] curr = get_tee((state[2], state[3]), state[4]) kp = get_element_by_idx(data_v1, \"keypoint\", 0)[-1] plot_kp(np.array(curr.exterior.coords), np.array(get_goal().exterior.coords), kp) In\u00a0[7]: Copied! <pre>def generate_videos(data, output_dir: str):\n    os.makedirs(output_dir, exist_ok=True)\n    prev = 0\n    episode_ends = data[\"meta\"][\"episode_ends\"][:]\n    for i in tqdm(range(episode_ends.shape[0])):\n        curr = int(episode_ends[i])\n        imgs = data[\"data\"][\"img\"][prev:curr]\n        imgs = np.array(imgs, dtype=np.uint8)\n        imageio.mimsave(os.path.join(output_dir, f\"gt_{i}.gif\"), imgs, fps=30)\n        prev = curr\n\n\ngenerate_videos(data_v1, \"../../data/data_v1_videos\")\ngenerate_videos(data_v2, \"../../data/data_v2_videos\")\n</pre> def generate_videos(data, output_dir: str):     os.makedirs(output_dir, exist_ok=True)     prev = 0     episode_ends = data[\"meta\"][\"episode_ends\"][:]     for i in tqdm(range(episode_ends.shape[0])):         curr = int(episode_ends[i])         imgs = data[\"data\"][\"img\"][prev:curr]         imgs = np.array(imgs, dtype=np.uint8)         imageio.mimsave(os.path.join(output_dir, f\"gt_{i}.gif\"), imgs, fps=30)         prev = curr   generate_videos(data_v1, \"../../data/data_v1_videos\") generate_videos(data_v2, \"../../data/data_v2_videos\")  <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 103/103 [00:17&lt;00:00,  5.87it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 206/206 [00:32&lt;00:00,  6.37it/s]\n</pre> In\u00a0[\u00a0]: Copied! <pre>def gif_to_base64(gif_path):\n    \"\"\"Convert GIF to base64 string\"\"\"\n    try:\n        with open(gif_path, \"rb\") as f:\n            return base64.b64encode(f.read()).decode()\n    except FileNotFoundError:\n        print(f\"Warning: {gif_path} not found\")\n        return None\n\n\ndef display_embedded_gif_matrix(gif_paths, rows, cols):\n    \"\"\"\n    Display GIFs embedded as base64.\n    \"\"\"\n    html_content = \"\"\"\n    &lt;div class=\"gif-matrix\" style=\"text-align: center;\"&gt;\n    &lt;style&gt;\n    .gif-matrix table { margin: 0 auto; border-collapse: collapse; }\n    .gif-matrix td { padding: 10px; text-align: center; }\n    .gif-matrix img { border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); }\n    &lt;/style&gt;\n    &lt;table&gt;\n    \"\"\"\n\n    for i in range(rows):\n        html_content += \"&lt;tr&gt;\"\n        for j in range(cols):\n            idx = i * cols + j\n            if idx &lt; len(gif_paths):\n                b64_gif = gif_to_base64(gif_paths[idx])\n                if b64_gif:\n                    html_content += f'''\n                    &lt;td&gt;\n                        &lt;img src=\"data:image/gif;base64,{b64_gif}\"\n                             style=\"width:200px;height:200px;\"\n                             alt=\"Animation {idx+1}\"&gt;\n                    &lt;/td&gt;\n                    '''\n                else:\n                    html_content += '&lt;td&gt;&lt;div style=\"width:200px;height:200px;background:#f0f0f0;\"&gt;&lt;/div&gt;&lt;/td&gt;'\n            else:\n                html_content += '&lt;td&gt;&lt;/td&gt;'\n        html_content += \"&lt;/tr&gt;\"\n\n    html_content += \"&lt;/table&gt;&lt;/div&gt;\"\n    return HTML(html_content)\n\n\ndef get_gifs(path: str, indices: Optional[Sequence[int]] = None) -&gt; list[str]:\n    all_gifs = sorted(os.path.join(path, gif) for gif in os.listdir(path) if gif.endswith(\".gif\"))\n    if indices is None:\n        indices = set(range(len(all_gifs)))\n    else:\n        indices = set(indices)\n\n    output = []\n    for i, f in enumerate(all_gifs):\n        if i not in indices:\n            continue\n        output.append(f)\n\n    return output\n</pre> def gif_to_base64(gif_path):     \"\"\"Convert GIF to base64 string\"\"\"     try:         with open(gif_path, \"rb\") as f:             return base64.b64encode(f.read()).decode()     except FileNotFoundError:         print(f\"Warning: {gif_path} not found\")         return None   def display_embedded_gif_matrix(gif_paths, rows, cols):     \"\"\"     Display GIFs embedded as base64.     \"\"\"     html_content = \"\"\"           \"\"\"      for i in range(rows):         html_content += \"\"         for j in range(cols):             idx = i * cols + j             if idx &lt; len(gif_paths):                 b64_gif = gif_to_base64(gif_paths[idx])                 if b64_gif:                     html_content += f'''                                           '''                 else:                     html_content += ''             else:                 html_content += ''         html_content += \"\"      html_content += \"\"     return HTML(html_content)   def get_gifs(path: str, indices: Optional[Sequence[int]] = None) -&gt; list[str]:     all_gifs = sorted(os.path.join(path, gif) for gif in os.listdir(path) if gif.endswith(\".gif\"))     if indices is None:         indices = set(range(len(all_gifs)))     else:         indices = set(indices)      output = []     for i, f in enumerate(all_gifs):         if i not in indices:             continue         output.append(f)      return output  In\u00a0[9]: Copied! <pre>display_embedded_gif_matrix(get_gifs(\"data_analysis/assets/data_v1_videos\")[:9], 3, 3)\n</pre> display_embedded_gif_matrix(get_gifs(\"data_analysis/assets/data_v1_videos\")[:9], 3, 3) Out[9]: In\u00a0[10]: Copied! <pre>display_embedded_gif_matrix(get_gifs(\"data_analysis/assets/data_v2_videos\")[:9], 3, 3)\n</pre> display_embedded_gif_matrix(get_gifs(\"data_analysis/assets/data_v2_videos\")[:9], 3, 3) Out[10]:"},{"location":"experiments/data_analysis/#dataset-analysis","title":"Dataset Analysis\u00b6","text":"<p>This notebook contains my analysis of the datasets v1 and v2. It was used to discover the differences between the two datasets.</p>"},{"location":"experiments/data_analysis/#background","title":"Background\u00b6","text":"<p>During my initial test runs of training, the performance of the model trained on v1 was better than the one trained on v2. However, the size of v2 was larger than v1, roughly 2x. Therefore, I would like to understand the differences between the two datasets and what might lead to such performance differences. Mainly, I am focusing on the following factors:</p> <ul> <li>Quality<ul> <li>Success rate/reward</li> <li>Smoothness of trajectories</li> </ul> </li> <li>Size<ul> <li>Number of episodes</li> <li>Length of each episode</li> </ul> </li> </ul> <p>If there are no significant differences in these factors, then maybe the test runs were too short to utilize the full potential of the datasets.</p>"},{"location":"experiments/data_analysis/#preparations","title":"Preparations\u00b6","text":""},{"location":"experiments/data_analysis/#finding-identicals","title":"Finding Identicals\u00b6","text":"<p>First of, I would like to find out whether there are any overlaps between the two datasets. This is done in a rather brute-force way: by comparing the images/videos of the two datasets. If the visualizations are identical, then it can be concluded that the episodes are identical.</p>"},{"location":"experiments/data_analysis/#quality-analysis","title":"Quality Analysis\u00b6","text":"<p>Next, I would like to assess the quality of the two datasets by computing the reward and success rate of the episodes.</p> <p>According to the original repository, the definition of success of an episode is determined by the area intersection of the goal pose and the current pose. In other words, if the overlapping area reaches a threshold (95% in this case), the episode is considered successful. Similarly, the reward is defined over the ratio of the overlapping area, but divided by the success threshold and clipped to 0-1. Hence, once the overlapping area reaches 95%, the reward reaches its maximum, and the model is allowed to terminate.</p> <p>In the following section, the average of the maximum rewards of each episode within both datasets are calculated, along with the corresponding success rates. In addition, I isolated the v2-exclusive episodes to get a clearer picture.</p>"},{"location":"experiments/data_analysis/#visualization","title":"Visualization\u00b6","text":"<p>Last but not least, I would like to visualize the trajectories of the two datasets.</p>"},{"location":"experiments/evaluation/","title":"Evaluation","text":""},{"location":"experiments/evaluation/#overview","title":"Overview","text":"<p>The evaluation of the trained policies is mainly done via the <code>eval.py</code> script from <code>real-stanford/diffusion_policy</code>. Following the approach described in the paper, I evaluated the last 10 checkpoints (saved every 50 epochs) across 50 environment initailizations. Due to resource limitations, only one policy model is trained instead of three from different training seeds for each experiment setup. The mean score, coverate, and success rate are calculated and reported using the following command:</p> <pre><code>python scripts/eval.py --exp PATH_TO_OUTPUT\n</code></pre>"},{"location":"experiments/evaluation/#issue-with-evaluation-metric","title":"Issue with Evaluation Metric","text":"<p>A mismatch is discovered between the metrics reported in the paper and from the evaluation script. In the original paper, the Push-T task uses \"target area converage\" as the metric. On the other hand, the metric provided by the evaluation script is mean score.</p> <p>The definition of mean score is the average of the maximum rewards in each episode. The reward is calculated by the intersection area of the goal and block pose over the goal area, then again divided by the success threshold (0.95 in this case) and clipped to \\([0, 1]\\). Refer to the following pseudo code for implementation:</p> <pre><code>intersection_area = goal_geom.intersection(block_geom).area\ngoal_area = goal_geom.area\ncoverage = intersection_area / goal_area\nreward = np.clip(coverage / self.success_threshold, 0, 1)\ndone = coverage &gt; self.success_threshold\n</code></pre> <p>The problem is that the <code>clip</code> operation is nonlinear, hence one cannot calculate the average target area converage from the reward or mean score along. In addition, the coverage metric cannot be retrieved without modifying the environment runner source code. This results in confusion on whether the evaluation script provided was used to calculate the metrics reported. In order to align with the paper, an additional patch is applied to calculate both target area coverage and mean score.</p>"},{"location":"experiments/evaluation/#results","title":"Results","text":"<p>The evaluation results of the policies are provided in the table below, along with the training curves as well.</p> Observation Dataset Model Mean Score Coverage Success Rate Time Taken Keypoints v1 Transformer 0.9472 0.9058 0.6540 6h 17m Keypoints v2 Transformer 0.8329 0.7953 0.5740 6h 55m Keypoints v1 CNN 0.9079 0.8719 0.7240 7h 17m Keypoints v2 CNN 0.8184 0.7826 0.5160 8h 11m Image v1 CNN 0.7995 0.7643 0.3700 31h 52m Image v2 CNN 0.8296 0.7937 0.4840 41h 05m Obervation Model Dataset V1 Dataset V2 Keypoints Transformer Keypoints CNN Image CNN"},{"location":"experiments/evaluation/#discussions","title":"Discussions","text":""},{"location":"experiments/evaluation/#training-dynamics","title":"Training Dynamics","text":"<p>From the training dynamics plot provided above, a weird pattern is observed. In keypoint-based experiments, training curves present an overfitting-like pattern, where the scores are high at approximately the 1000th epochs, but then dropped to a lower level. Likewise, validation curves also tend to decay in the second half of the training, though the drop is less significant than the training ones. Image-based experiments, on the other hand, present a more \"expected\" training dynamics, where the scores rise quickly at the beginning and then gradually stabilize. Also note that the variance of the scores is much higher than the keypoint-based experiments.</p> <p>Overfitting is usually detected by the decaying validation score. However, in the keypoint-based experiments, it is the training score that decays. One possible reason is that the training set evaluation has a lower number of environments, which is 7 compared to 50 in the test set. This would lead to a higher sensitivity to noise and randomness, and thus a higher variance in the training score, which matches the observation.</p>"},{"location":"experiments/evaluation/#observation","title":"Observation","text":"<p>Policies trained on image-based observations perform worse than the ones trained on keypoint-based observations, which matches the observation in the paper. Intuitively, keypoints can be understood as an intermediate representation or a feature of the image. Models without needing to learn the image representation can focus more on learning the policy, thus achieving better results.</p>"},{"location":"experiments/evaluation/#dataset","title":"Dataset","text":"<p>In the data analysis notebook, the properties of the two datasets can be summarized as:</p> <ul> <li>Size: v2 is double the size of v1.</li> <li>Collection: v1 is a subset of v2.</li> <li>Quality: both datasets have similar quality in terms of reward and success rate.</li> </ul> <p>Thus, the most significant difference between the two is the size.</p> <p>Naturally, it is expected that more data leads to better generalization and better results. However, the experiment results contradict with the aforementioned claim. In keypoint-based experiments, policies trained with v1 performed better than v2. On the other hand, image-based experiments matches the scaling law. One possible reason is the overfitting pattern discussed in previous sections. Even if the dataset size is doubled, the number of training episodes is still low, 103 and 206 respectively. The large number of training epochs might easily lead to overfitting regardless of the dataset size difference. Evaluation on the best-performing checkpoints instead of the last ones might provide additional insights to this.</p>"},{"location":"experiments/evaluation/#model-architecture","title":"Model Architecture","text":"<p>Comparison between model architectures can be done via looking at the keypoint-based experiments. In the paper, the CNN models have a slight lead over the Transformer models in the Push-T task. However, my experiment results contradict with the observation. Further investigation is needed to understand the reason behind this.</p>"},{"location":"experiments/evaluation/#conclusion-and-future-work","title":"Conclusion and Future Work","text":"<p>This work is an attempt to reproduce the results in the paper \"Diffusion Policy: Visuomotor Policy Learning via Action Diffusion\". The evaluation results are worse than the numbers reported in the paper, and several inconsistencies are discovered. However, due to the time and resource constraints, the experiments are not able to provide a comprehensive analysis. Future work can be done to investigate the following aspects:</p> <ul> <li>Training on the original dataset instead of the custom one.</li> <li>Evaluation on the best-performing checkpoints instead of the last ones.</li> </ul>"},{"location":"experiments/training/","title":"Training","text":""},{"location":"experiments/training/#overview","title":"Overview","text":"<p>Training is mainly done via the <code>train.py</code> script from <code>real-stanford/diffusion_policy</code>. Minor modifications are made to the training configuration to match the experimental settings described in the paper, and can be found in the corresponding shell scripts.</p> <pre><code>python scripts/train_lowdim_transformer_v1.sh\npython scripts/train_lowdim_transformer_v2.sh\npython scripts/train_img_cnn_v1.sh\npython scripts/train_img_cnn_v2.sh\npython scripts/train_lowdim_cnn_v1.sh\npython scripts/train_lowdim_cnn_v2.sh\n</code></pre>"},{"location":"experiments/training/#hardware","title":"Hardware","text":"<ul> <li>RTX A6000</li> </ul>"},{"location":"logs/","title":"All Work Logs","text":""},{"location":"logs/2025/08/06/day-1/","title":"Day 1","text":""},{"location":"logs/2025/08/06/day-1/#work-done","title":"Work Done","text":""},{"location":"logs/2025/08/06/day-1/#test-evaluation-script-and-environment","title":"Test Evaluation Script and Environment","text":"<p>Ran the evaluation command from <code>lerobot/diffusion_policy</code>.</p> <pre><code>python -m lerobot.scripts.eval --policy.path=lerobot/diffusion_pusht --output_dir ./output --env.type=pusht --eval.n_episodes=500 --eval.batch_size=50\n</code></pre> <p>And the results:</p> Mine lerobot/diffusion_pusht Paper Average max. overlap ratio 0.962 0.955 0.957 Success rate for 500 episodes (%) 64.2 65.4 64.2"},{"location":"logs/2025/08/06/day-1/#dataset-discovery","title":"Dataset Discovery","text":"<p>I opened up a jupyter notebook playground and fiddled with the data a little bit. Here's the structure of the data with <code>zarr.open_group(path).tree()</code>:</p> <pre><code>/\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 action (N, 2) float32\n\u2502   \u251c\u2500\u2500 img (N, 96, 96, 3) float32\n\u2502   \u251c\u2500\u2500 keypoint (N, 9, 2) float32\n\u2502   \u251c\u2500\u2500 n_contacts (N, 1) float32\n\u2502   \u2514\u2500\u2500 state (N, 5) float32\n\u2514\u2500\u2500 meta\n    \u2514\u2500\u2500 episode_ends (K,) int64\n</code></pre> <p>Initiallly, I compared it to the <code>lerobot/pusht</code> dataset released by HuggingFace. However, the entries are so different that it's difficult to match them. I printed the arrays, displayed the images, trying to get a sense of what those values mean. Here's my attempt:</p> <ul> <li><code>episode_ends</code> marks the ending scene/index of each episode. Use this to split the data into K rounds and label them with episode indices from 0 to K - 1.</li> <li><code>state</code>: I make my assumptions by simultaneously looking at the corresponding image.<ul> <li>The first two numbers are the position of the tooltip.</li> <li>3rd &amp; 4th are the positions of the T-shaped object.</li> <li>5th looks like the orientation of the object in radian.</li> </ul> </li> <li><code>img</code> visualizes the current state (potentially given the <code>keypoint</code>s and <code>state</code>s).</li> </ul> <p>At some point I came up with the idea that I should also check the dataset released by the authors of the original paper. BINGO!</p>"},{"location":"logs/2025/08/06/day-1/#real-stanforddiffusion_policy","title":"<code>real-stanford/diffusion_policy</code>","text":"<p>Naturally, the next step is to discover the diffusion policy paper and code. Their README suggests running the notebook in colab, but I failed to open it due to some issues. I then turned to the example commands in the README.</p> <p>I started with the low-dimension setup. Using the exact configuration from the paper, my results (0.944@750 and 0.948@950) seemed to match the authors' checkpoints. However, this is entirely based on the name of the checkpoint. Further investigation is required to determine whether this is a successful reproduction.</p>"},{"location":"logs/2025/08/06/day-1/#wip","title":"WIP","text":"<ul> <li>Reproducing both image and low-dimension experiments.</li> <li>Naively training on custom v1 dataset with only swapping the dataset itself.</li> </ul>"},{"location":"logs/2025/08/06/day-1/#todo","title":"TODO","text":"<p>Focus on <code>real-stanford/diffusion_policy</code>.</p> <ul> <li>Look into colab notebooks.</li> <li>Training understanding<ul> <li>Policy input/output.</li> <li>How is a diffusion model trained?</li> <li>Hyperparameters.</li> </ul> </li> <li>Evaluation understanding:<ul> <li>How do validation and test work?</li> <li>Definition of metrics: success rate, reward.</li> </ul> </li> <li>PushT environment: compare it to <code>lerobot/gym-pusht</code>.</li> <li>Data preprocessing<ul> <li>How image and low-dimension tasks utilize data.</li> <li>Convert to lerobot-style dataset?</li> </ul> </li> <li>Setup local <code>wandb</code>?</li> <li>Code cleanup and commit.</li> </ul>"},{"location":"logs/2025/08/06/day-1/#random-notes","title":"Random Notes","text":""},{"location":"logs/2025/08/06/day-1/#attributeerror-space-object-has-no-attribute-add_collision_handler","title":"AttributeError: 'Space' object has no attribute 'add_collision_handler'","text":"<p>Looks like <code>pymunk</code> removed the method after version 7.0. Running <code>uv add 'pymunk&lt;7'</code> solves the issue.</p>"},{"location":"logs/2025/08/06/day-1/#environment-preparation-for-diffusion_policy","title":"Environment Preparation for <code>diffusion_policy</code>","text":"<p>Setting up the environment wasn't the easiest. This is a two-year-old project. Huggingface libraries have been moving fast and not afraid of breaking things. Python's dependency management via <code>conda</code> and <code>pip</code> isn't the best<sup>1</sup>. All three factors lead to hours of fixing module/attribute not found errors and nonexistence of valid version combinations. Eventually, I had a fragile but working environment. Time for running some code!</p> <ol> <li> <p>Let's hope for uv!\u00a0\u21a9</p> </li> </ol>"},{"location":"logs/2025/08/07/day-2/","title":"Day 2","text":""},{"location":"logs/2025/08/07/day-2/#work-done","title":"Work Done","text":""},{"location":"logs/2025/08/07/day-2/#training-with-real-stanforddiffusion_policy","title":"Training with <code>real-stanford/diffusion_policy</code>","text":"<p>Ran two experiment setups using the <code>real-stanford/diffusion_policy</code> repository.</p> <ol> <li>Transformer + state-based observations.</li> <li>UNet + image-based observations.</li> </ol> <p>Both configurations were trained on two datasets, making a 2x2 matrix. Most default settings were adopted, except for the number of epochs and learning rate scheduler. The number fo epochs was set to 1000 for all cases in order to get a quick taste, and the learning rate scheduler is set to constant to make sure the model goes far enough.</p> <p>Insert table</p>"},{"location":"logs/2025/08/07/day-2/#analysis","title":"Analysis","text":"<p>Interestingly, models trained on dataset v1 both outperformed the ones trained on dataset v2. Dataset v2 is roughly double the size of v1, thus I initially thought scaling law would also work. Two potential reasons:</p> <ul> <li>Quality in data: maybe v2 yields lower quality?</li> <li>Larger dataset requires longer training time.</li> </ul> <p>On the other hand, state-based model outperforms image-based model. I think this is expected since intuitively there definitely will be estimation errors when using vision.</p>"},{"location":"logs/2025/08/07/day-2/#wip","title":"WIP","text":"<ul> <li>Still fighting with environment setup \ud83d\ude22.<ul> <li>Currently maintaining two environments: one for <code>diffusion_policy</code>, the other for my repository.</li> </ul> </li> <li>Change of plan again.<ul> <li>Don't want to fork <code>diffusion_policy</code> \\(\\to\\) temporarily use git submodule instead for reproduction purposes.</li> <li>Tried to install <code>diffusion_policy</code> with <code>pip</code> or <code>conda</code> but have no luck. The flat layout and the lack of <code>__init__.py</code> prohibited me from importing it as a module.</li> <li>Colab notebook is almost a self-contained training + evaluation code. Adopt that but instead structure it into a <code>tiny_dp</code> python submodule.</li> </ul> </li> </ul>"},{"location":"logs/2025/08/07/day-2/#todo","title":"TODO","text":"<ul> <li>Look into colab notebooks.</li> <li>Training understanding<ul> <li>Policy input/output.</li> <li>How is a diffusion model trained?</li> <li>Hyperparameters.</li> </ul> </li> <li>Evaluation understanding:<ul> <li>How do validation and test work?</li> <li>Definition of metrics: success rate, reward.</li> </ul> </li> <li>PushT environment: compare it to <code>lerobot/gym-pusht</code>.</li> <li>Data preprocessing<ul> <li>How image and low-dimension tasks utilize data.</li> <li>Convert to lerobot-style dataset?</li> </ul> </li> </ul>"},{"location":"logs/2025/08/08/day-3/","title":"Day 3","text":""},{"location":"logs/2025/08/08/day-3/#work-done","title":"Work Done","text":"<ul> <li>Dataset analysis: played with both versions of dataset in <code>data_analysis.ipynb</code>.</li> <li><code>diffusion_policy</code><ul> <li>Successfully installed and imported as a package after being patched with necessary <code>__init__.py</code> files.</li> <li>Understood success rate and metrics and how evaluation works.</li> </ul> </li> </ul>"},{"location":"logs/2025/08/08/day-3/#wip","title":"WIP","text":"<ul> <li>Still figuring out how this repository should look like. Feel like I'd only need some basic python scripts to launch the training and evaluation jobs. Then why bother installing the original repo?</li> </ul>"},{"location":"logs/2025/08/08/day-3/#todo","title":"TODO","text":"<ul> <li>Look into colab notebooks.</li> <li>Training understanding<ul> <li>Policy input/output.</li> <li>How is a diffusion model trained?</li> <li>Hyperparameters.</li> </ul> </li> <li>Evaluation understanding: how does validation work?</li> <li>PushT environment: compare it to <code>lerobot/gym-pusht</code>.</li> <li>Data preprocessing<ul> <li>How image and low-dimension tasks utilize data.</li> </ul> </li> </ul>"},{"location":"logs/2025/08/11/day-4/","title":"Day 4","text":""},{"location":"logs/2025/08/11/day-4/#work-done","title":"Work Done","text":"<ul> <li>Evaluated policies from full training runs.</li> <li>Completed dataset analysis notebook.</li> </ul>"},{"location":"logs/2025/08/11/day-4/#wip","title":"WIP","text":"<ul> <li>Training CNN-based policy model on state-based dataset.</li> </ul>"},{"location":"logs/2025/08/11/day-4/#todo","title":"TODO","text":"<ul> <li>Training<ul> <li>Compare results between CNN and transformer-based policy models.</li> <li>Plot training curves from json log.</li> <li>Analyze training results. Compare with original paper.</li> </ul> </li> <li>Documentation<ul> <li>Methodology section: diffusion model training, evaluation.</li> <li>Data preprocessing: how image and low-dimension tasks utilize data.</li> </ul> </li> </ul>"},{"location":"logs/2025/08/12/day-5/","title":"Day 5","text":""},{"location":"logs/2025/08/12/day-5/#work-done","title":"Work Done","text":"<ul> <li>Trained and evaluated policies from full training runs.</li> <li>Plotted training curves from json log.</li> </ul>"},{"location":"logs/2025/08/12/day-5/#wip","title":"WIP","text":"<ul> <li>Analyzing training results.</li> <li>Writing documentation.</li> </ul>"},{"location":"logs/2025/08/12/day-5/#todo","title":"TODO","text":"<ul> <li>Complete documentation and wrap-up.</li> <li>Understanding<ul> <li>Methodology section: diffusion model training, evaluation.</li> <li>Data preprocessing: how image and low-dimension tasks utilize data.</li> </ul> </li> </ul>"},{"location":"logs/2025/08/13/day-6/","title":"Day 6","text":""},{"location":"logs/2025/08/13/day-6/#work-done","title":"Work Done","text":"<ul> <li>Patched original codebase and re-ran evaluation.</li> <li>Completed analysis of training results.</li> <li>Completed documentation.</li> </ul>"},{"location":"logs/2025/08/05/project-start/","title":"Project start","text":""},{"location":"logs/2025/08/05/project-start/#work-done","title":"Work Done","text":"<ul> <li>Watched the youtube video of diffusion policy presentation. My takeaways:<ul> <li>Human's vision reaction time is ~300ms. If the training data is collected by human, each action sequence/chunk should be at the same order of magnitudes to that<sup>1</sup>.</li> <li>Diffusion policy works well both in joint-space and action-space. However, working in action space requires a good IK.</li> </ul> </li> <li>Created repository.</li> <li>Draft plan:<ul> <li><code>huggingface/lerobot</code>: start from the training and evaluation scripts there. Maybe reproduce <code>lerobot/diffusion_pusht</code> if feasible.</li> <li>Per request, use <code>huggingface/gym-pusht</code> for simulation environment.</li> <li>Maybe Material for MkDocs for documentation and report.</li> <li>Or maybe just the paper-style, good-old \\(\\LaTeX\\).</li> <li><code>uv</code> for package management? Not sure if this would work since most of the environment requires <code>conda</code>/<code>mamba</code> for non-python dependencies.</li> <li><code>marimo</code> or <code>jupyter notebook</code> for interactive sessions? Or use the jupyter notebook extension for mkdocs.</li> </ul> </li> </ul>"},{"location":"logs/2025/08/05/project-start/#todo","title":"TODO","text":"<ul> <li>Understand difference between DDPM and DDIM<sup>2</sup>.</li> <li>Fiddle with <code>lerobot/diffusion_pusht</code>.<ul> <li>Understand the workflow.</li> <li>Get a feeling of how resource-hungry are the training &amp; evaluation scripts.</li> </ul> </li> <li>Discover the custom pusht dataset.</li> <li>Perhaps read the paper?</li> </ul> <ol> <li> <p>During the Q&amp;A session at 51:18 \u21a9</p> </li> <li> <p>Mentioned during the final Q&amp;A regarding speed optimizations.\u00a0\u21a9</p> </li> </ol>"},{"location":"logs/archive/2025/","title":"2025","text":""}]}